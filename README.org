#+title: Readme
* Requirements
- Julia ver 1.11.0 (2024-10-07)
* About this program
This is a multi-armed bandit problems simulator that is designed for machine learning simulations (algorithm evaluation), as well as cognitive scientific analysis (parameter and model estimation and data fit).
#+begin_src sh
julia -t 4 bandit1.jl
#+end_src
or
#+begin_src sh
julia -t 4 q-heatmap.jl
#+end_src
will show some result. Or probably it's better to
#+begin_src sh
julia -t 4
#+end_src
and then
#+begin_src sh
include("q-heatmap.jl")
#+end_src
or something, I guess?
* The design
Usually, a reinforcement learning system (with a single state) consists of a pair of agent and environment:
#+begin_src ascii
#########            #########
#       # --action-> #       #
# agent #            #  env  #
#       # <-reward-- #       #
#########            #########
#+end_src
In this program, a System has an Agent and an Environment.
An Agent has two things:
a Policy (such as epsilon-greedy method) and a ActionValueEstimator (such as Q-values with the learning parameter Î±).
It also has a History that records the sequences of action selections and rewards, and other information.
The History can turn into an Evaluation that calculates the performance and behavior measures
of the Agent in relation to the Environment.

The separation of Agent into a policy and an estimator enabled
combinations of them, such as Softmax with Q-values and Softmax with
sample average estimator. 
* To-do
There's a lot do to.
- Parameter-performance analysis with heatmaps (`q-heatmap.jl` is the only one for now)
- Implementation of RS, Softsatisficing and more descriptive satisficing models.
  - RS can be used with Q-values, not just with the average reward estimator, which could even make the likelihoood analysis smoother.
- Parameter recovery analysis, and how to implement it into the current design.
- Data fit with the actual bandit problem experiment data.
* A Mermaid diagram
#+begin_src mermaid
#+begin_src mermaid
classDiagram
    class AbstractActionValueEstimator["AbstractActionValueEstimator"]
    class SampleAverageEstimator["SampleAverageEstimator"]
    class QEstimator["QEstimator"]
    class DLREstimator["DLREstimator"]
    class ThompsonSamplingEstimator["ThompsonSamplingEstimator"]
    class AbstractPolicy["AbstractPolicy"]
    class EpsilonGreedyPolicy["EpsilonGreedyPolicy"]
    class SoftmaxPolicy["SoftmaxPolicy"]
    class UCBPolicy["UCBPolicy"]
    class UCBTunedPolicy["UCBTunedPolicy"]
    class SatisficingPolicy["SatisficingPolicy"]
    class ThompsonSamplingPolicy["ThompsonSamplingPolicy"]
    class AbstractEnvironment["AbstractEnvironment"]
    class Environment["Environment"]
    class AbstractHistory["AbstractHistory"]
    class History["History"]
    class HistoryRich["HistoryRich"]
    class AbstractSystem["AbstractSystem"]
    class System["System"]
    class SystemRich["SystemRich"]
    class Agent["Agent"]

    AbstractActionValueEstimator <|-- SampleAverageEstimator
    AbstractActionValueEstimator <|-- QEstimator
    AbstractActionValueEstimator <|-- DLREstimator
    AbstractActionValueEstimator <|-- ThompsonSamplingEstimator
    AbstractPolicy <|-- EpsilonGreedyPolicy
    AbstractPolicy <|-- SoftmaxPolicy
    AbstractPolicy <|-- UCBPolicy
    AbstractPolicy <|-- UCBTunedPolicy
    AbstractPolicy <|-- SatisficingPolicy
    AbstractPolicy <|-- ThompsonSamplingPolicy
    AbstractEnvironment <|-- Environment
    AbstractHistory <|-- History
    AbstractHistory <|-- HistoryRich
    AbstractSystem <|-- System
    AbstractSystem <|-- SystemRich
    Agent --> AbstractActionValueEstimator
    Agent --> AbstractPolicy
    System --> Agent
    System --> Environment
    System --> AbstractHistory
    SystemRich --> Agent
    SystemRich --> Environment
    SystemRich --> AbstractHistory
#+end_src


#+begin_src mermaid
flowchart LR
    subgraph Universe
        subgraph System
                 subgraph Agent
                          Policy
                          Estimator
                 end
                 History
                 Environment
        end
    end
#+end_src
